\subsection{The likelihood function}

    The model-independent fit to the \ac{mc}-generated data set, $D = \Set{d_1, d_2,\dots, d_m}$, is performed through the maximum-likelihood method.
    In this section, I will discuss the expression of the likelihood of the model that gets maximized in the fitting routine.


    Let $p \coloneqq (p_1, p_2,\dots, p_n)$ be the fit-parameter vector; then, Bayes' theorem reads
    \begin{equation}
        P(p\given D) = \frac{P(D\given p)\, P(p)}{P(D)}.
    \end{equation}
    Now, as the fit parameters are independent on each other, their probabilities factorize such that
    \begin{equation}
        P(p) = P(p_1,\dots,p_n) = \prod_{i = 1}^n P_i(p_i),
    \end{equation}
    $P_i$ being the probability density function of the $i$-th parameter.
    Since there is no information about the fit parameters, their prior distribution is constant within the parameters' ranges:
    \begin{equation}\label{eq:flat_priors}
        P_i(p_i) =
        \begin{cases}
            1/(b_i - a_i) & \text{if } a_i \le p_i \le b_i, \\
            0             & \text{otherwise},
        \end{cases}
    \end{equation}
    with $[a_i, b_i]$ the domain of the $i$-th parameter.
    Therefore, the prior, $P(p_1,\dots,p_n)$, is a constant function in the hypercube where all its arguments are defined.


    The likelihood function is defined as the conditional probability of observing the data set $D$ given $p$, that is
    \begin{equation}
        L(p) \coloneqq P(D\given p).
    \end{equation}
    Since the priors and $P(D)$ are constant functions, the likelihood is directly proportional to the posterior probability:
    \begin{equation}
        P(p\given D) = \alpha L(p),\quad \text{with } \alpha \coloneqq \frac{P(p)}{P(D)}.
    \end{equation}
    The logarithm of the likelihood reads
    \begin{equation}
        \log L(p) = \log P(p\given D) - \log \alpha.
    \end{equation}


    The posterior probability of the fit parameters given an observed point in the phase space, $d$, is proportional to the model intensity\index{intensity}:
    \begin{equation}
        P(p\given d) = \frac{\Intensity(d)}{\displaystyle\int \Intensity(\tau)\ud\tau},
    \end{equation}
    where the normalizing integral is over the whole decay phase-space.
    Assuming the point in the observed data set are uncorrelated, the posterior given the whole set $D$ is
    \begin{equation}
        P(p\given D) = \prod_{d\in D} P(p\given d) = \frac{\displaystyle\prod_{d\in D}\Intensity(d)}{\displaystyle\left(\int \Intensity(\tau)\ud\tau\right)^m}.
    \end{equation}
    The log-likelihood then reads
    \begin{equation}
        \log L(p) = \sum_{d\in D}\log \Intensity(d) - m\log\int \Intensity(\tau) \ud \tau - \log \alpha.
    \end{equation}
    Retaining only the $p$-dependent terms in the equation above, the expression of the log-likelihood that needs to be maximized is
    \begin{equation}\label{eq:log_lik_unstable}
        \log \tilde L(p) = \sum_{d\in D}\log \Intensity(d) - m\log \int \Intensity(\tau)\ud\tau.
    \end{equation}


    It is worth pointing out that, in the fit routine, the expression used for the log-likelihood is the following:
    \begin{equation}\label{eq:log_lik_stable}
        \log \tilde L(p) = \sum_{d\in D}\left(\log \Intensity(d) - \log \int \Intensity(\tau)\ud\tau\right)\!.
    \end{equation}
    In fact, while being mathematically equivalent, equation~\eqref{eq:log_lik_stable} is numerically more stable than equation~\eqref{eq:log_lik_unstable}.
